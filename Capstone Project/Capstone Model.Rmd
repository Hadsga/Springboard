---
title: "Capstone Model"
output: md_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
load("C:/Users/X1/Desktop/Springboard Data/Yelp/yelp.Rda")
load("C:/Users/X1/Desktop/Springboard Data/Yelp/cp.Rdata")
```


####Packages
```{r, warning=FALSE, message=FALSE}

library(tidyverse)
library(mlr)
library(mlrMBO)
library(parallelMap)
library(parallel)
library(data.table)
library(caret)

```


#Goal of the model

The goal of the model is to predict review counts for Asian restaurants by the restaurant categories and their attributes and city. 
```{r}

glimpse(yelp)

```

#1 Model Building

## 1.1 Subsetting the the data

Selecting the columns categories and attributes which were built through data wrangling (leaving out columns with less than two factor levels). 
```{r, eval=FALSE}

dat = yelp %>%
  dplyr::select(review_count, Alcohol:lunch, -no_music)

```


## 1.2 Preparing the data

Making dummy features for all character columns. 
```{r, eval=FALSE}


dummy_dat = dummyVars("~ .", data = dat, fullRank = T)

dat = data.frame(predict(dummy_dat, newdata = dat))

```


## 1.3 Building the model

Building the model using XGBoost. 
```{r, eval=FALSE}

train_task = makeRegrTask(data = dat, target = "review_count")

xgb = makeLearner("regr.xgboost", 
                  par.vals = list(eta = 0.1))

ps = makeParamSet(
  makeIntegerParam("nrounds", lower = 1, upper = 500),
  makeIntegerParam("max_depth", lower = 1, upper = 6),
  makeNumericParam("subsample", lower = 0.2, upper = 1),
  makeNumericParam("colsample_bytree", lower = 0.2, upper = 1))


mbo.ctrl = makeMBOControl()
mbo.ctrl = setMBOControlTermination(mbo.ctrl, iters = 5)
ctrl = makeTuneControlMBO(mbo.control = mbo.ctrl)


rdesc = makeResampleDesc("CV",
                         iters = 5) 


parallelStartSocket(cpus = detectCores())

set.seed(3)
xgb_tune = tuneParams(learner = xgb, 
                      par.set = ps,
                      task = train_task, 
                      resampling = rdesc,
                      control = ctrl, 
                      show.info = FALSE, 
                      measures = rmse)

parallelStop()

```

Checking the tuned hyperparameters. 
```{r}

xgb_tune$x

```

## 1.4 Performance of the model

Checking the performance measures of the final model.
```{r, eval=FALSE}

lrn = setHyperPars(xgb, par.vals = xgb_tune$x)

parallelStartSocket(cpus = detectCores())

set.seed(3)
bmr = benchmark(lrn, train_task, rdesc, measures = list(rmse, rsq))

parallelStop()

```

```{r}

bmr$results$dat$regr.xgboost$aggr %>%
  knitr::kable("html", col.names = NULL)

```


## 1.5 Feature importance 

Checking the most important features (top 10) for predicting the review count.
```{r, eval=FALSE}


imp = generateFeatureImportanceData(task = train_task, 
                                    method = "permutation.importance", 
                                    learner = lrn,
                                    nmc = 10)

imp = melt(imp$res[, 2:ncol(imp$res)]) 

```


```{r}

imp %>% 
  arrange(-value) %>%
  head(10) %>%
  ggplot(aes(x = reorder(variable, value), y = value)) + 
  geom_bar(stat = "identity")  +
  labs(x = "Features", y = "Permutation Importance") +
  coord_flip() +
  theme_minimal()

```



##1.6 Partial Dependencies

Checking the partial dependencies of all features.  
```{r, eval=FALSE}

mod = mlr::train(lrn, train_task)

features1 = imp %>%
  arrange(-value) %>%
  slice(1:12)

pd1 = generatePartialDependenceData(mod, train_task, c(paste(features1$variable)))

features2 = imp %>%
  arrange(-value) %>%
  slice(13:24)

pd2 = generatePartialDependenceData(mod, train_task, c(paste(features2$variable)))


features3 = imp %>%
  arrange(-value) %>%
  slice(25:36)

pd3 = generatePartialDependenceData(mod, train_task, c(paste(features3$variable)))

features4 = imp %>%
  arrange(-value) %>%
  slice(37:48)

pd4 = generatePartialDependenceData(mod, train_task, c(paste(features4$variable)))

features5 = imp %>%
  arrange(-value) %>%
  slice(49:60)

pd5 = generatePartialDependenceData(mod, train_task, c(paste(features5$variable)))

features6 = imp %>%
  arrange(-value) %>%
  slice(61:72)

pd6 = generatePartialDependenceData(mod, train_task, c(paste(features6$variable)))

features7 = imp %>%
  arrange(-value) %>%
  slice(73:78)

pd7 = generatePartialDependenceData(mod, train_task, c(paste(features7$variable)))

```

```{r}

plotPartialDependence(pd1)
plotPartialDependence(pd2)
plotPartialDependence(pd3)
plotPartialDependence(pd4)
plotPartialDependence(pd5)
plotPartialDependence(pd6)
plotPartialDependence(pd7)

```



#2 Conclusion

- The model is able to predict review counts with an RMSE of 104.

- Overall, 37% of the variance can be explained. 

- The features which are most important for the prediction are diveyTrue, 
  dinnerTrue and BikeparkingTrue. 

- In order to get a high review count the restaurant should be take the 
  following points into account:
  - Food offers:
    - Dessert, lunch, brunch, alcohol, dairy-free options, vegan options,   
      soy-free options, kosher options.  
  - Non-Food offers:
    - Smoking area, happy hour, parking options (garage, valet, street), wheelchair 
      accessibility, counter service, table service, BYOB (for nonalcoholic 
      drinks). 
  - Ambiance:
    - Trendy, casual, good for late night, good for dinner. 








